{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0Yid3mm5qEJVb1PyPF9Cb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fashion-MNIST MLP From Scratch\n","\n","This notebook contains a full implementation of a Multi-Layer Perceptron (MLP) from scratch using PyTorch (without autograd), for classifying Fashion-MNIST."],"metadata":{"id":"fySKKTMMfwUd"}},{"cell_type":"markdown","source":["### Approach & MLP Architecture\n","\n","This project implements a Multi-Layer Perceptron (MLP) from scratch to classify images from the Fashion-MNIST dataset. The implementation strictly avoids using PyTorch’s autograd or high-level neural network modules such as `nn.Linear`, `nn.ReLU`, or `torch.optim`.\n","\n","#### Dataset Handling\n","- The Fashion-MNIST dataset is loaded using `torchvision.datasets`.\n","- A manual train-validation split (50,000 train / 10,000 validation) is performed from the training set.\n","- A `preprocess` function flattens each image (28x28 → 784) and normalizes it to the range [0, 1].\n","\n","#### MLP Design\n","- The MLP class is manually implemented.\n","- It supports configurable architectures with either one or two hidden layers.\n","- Weights and biases are initialized using `torch.randn` and `torch.zeros`.\n","- All forward and backward computations are implemented manually using tensor operations and the chain rule.\n","\n","#### Activation & Loss Functions\n","- ReLU is used for hidden layers.\n","- Softmax is used at the output layer.\n","- Cross-entropy loss is computed using its mathematical formula without any built-in loss function.\n","\n","#### Training\n","- The model is trained using mini-batch stochastic gradient descent (SGD).\n","- Training is performed for 5 epochs using a batch size and learning rate specified per configuration.\n","- A separate `evaluate()` function computes the final test accuracy.\n","\n","#### Hyperparameter Experiments\n","- Several configurations of hidden sizes, learning rates, and batch sizes were tested.\n","- Results are reported in a Markdown table with the best configuration clearly highlighted.\n","\n","This approach ensures a full manual implementation of MLP training and evaluation."],"metadata":{"id":"Ave5SCEKsxtO"}},{"cell_type":"markdown","source":["### 1. Dataset Preparation"],"metadata":{"id":"9QuiBVgah-9R"}},{"cell_type":"markdown","source":["### 1.1 Download and Split"],"metadata":{"id":"im3uCin3iA6a"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"qjQrf6F56iVp","executionInfo":{"status":"ok","timestamp":1748095888289,"user_tz":-330,"elapsed":108,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}}},"outputs":[],"source":["import torch\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import numpy as np\n","\n","# Step 1: Download the Fashion-MNIST training and test datasets\n","train_val_data = datasets.FashionMNIST(\n","    root='data',\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","test_data = datasets.FashionMNIST(\n","    root='data',\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","# Step 2: Manually split the training set into training and validation\n","train_size = 50000\n","val_size = len(train_val_data) - train_size\n","\n","# Set a seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Random split\n","train_data, val_data = torch.utils.data.random_split(train_val_data, [train_size, val_size])"]},{"cell_type":"markdown","source":["### 1.2 Preprocess Function"],"metadata":{"id":"kNe1iYbdisqx"}},{"cell_type":"code","source":["def preprocess(images):\n","    \"\"\"\n","    Flattens and normalizes a batch of image tensors.\n","\n","    Args:\n","        images (torch.Tensor): Batch of shape (batch_size, 1, 28, 28)\n","\n","    Returns:\n","        torch.Tensor: Batch of shape (batch_size, 784) with normalized pixel values\n","    \"\"\"\n","    # Flatten: from (batch_size, 1, 28, 28) to (batch_size, 784)\n","    flattened = images.view(images.shape[0], -1)\n","\n","    # Explicit normalization to [0, 1]\n","    normalized = flattened / 1.0  # Already in [0, 1], but still normalized explicitly\n","\n","    return normalized"],"metadata":{"id":"AugLJk_A7DBJ","executionInfo":{"status":"ok","timestamp":1748095893365,"user_tz":-330,"elapsed":19,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### 2. MLP Class and Functions"],"metadata":{"id":"qftBp117lTIH"}},{"cell_type":"code","source":["def relu(x):\n","    return torch.maximum(x, torch.zeros_like(x))\n","\n","def softmax(x):\n","    exps = torch.exp(x - torch.max(x, dim=1, keepdim=True).values)\n","    return exps / torch.sum(exps, dim=1, keepdim=True)\n","\n","def relu_derivative(x):\n","    return (x > 0).float()"],"metadata":{"id":"m4-QbeUo8DSW","executionInfo":{"status":"ok","timestamp":1748095895424,"user_tz":-330,"elapsed":15,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class MLP:\n","    def __init__(self, input_size, hidden_sizes, output_size):\n","        torch.manual_seed(42)\n","        self.hidden_sizes = hidden_sizes\n","        self.num_layers = len(hidden_sizes)\n","\n","        # Define weights and biases for 1 or 2 hidden layers\n","        self.weights1 = torch.randn(input_size, hidden_sizes[0]) * 0.01\n","        self.bias1 = torch.zeros(hidden_sizes[0])\n","\n","        if self.num_layers == 2:\n","            self.weights2 = torch.randn(hidden_sizes[0], hidden_sizes[1]) * 0.01\n","            self.bias2 = torch.zeros(hidden_sizes[1])\n","            self.weights3 = torch.randn(hidden_sizes[1], output_size) * 0.01\n","            self.bias3 = torch.zeros(output_size)\n","        else:\n","            self.weights2 = torch.randn(hidden_sizes[0], output_size) * 0.01\n","            self.bias2 = torch.zeros(output_size)\n","\n","    def forward(self, x):\n","        self.z1 = x @ self.weights1 + self.bias1\n","        self.a1 = relu(self.z1)\n","\n","        if self.num_layers == 2:\n","            self.z2 = self.a1 @ self.weights2 + self.bias2\n","            self.a2 = relu(self.z2)\n","            self.z3 = self.a2 @ self.weights3 + self.bias3\n","            self.a3 = softmax(self.z3)\n","            return self.a3\n","        else:\n","            self.z2 = self.a1 @ self.weights2 + self.bias2\n","            self.a2 = softmax(self.z2)\n","            return self.a2\n","\n","    def backward(self, x, y, learning_rate):\n","        batch_size = x.shape[0]\n","        y_one_hot = torch.zeros(batch_size, 10)\n","        y_one_hot[torch.arange(batch_size), y] = 1\n","\n","        if self.num_layers == 2:\n","            dz3 = self.a3 - y_one_hot\n","            dw3 = self.a2.T @ dz3 / batch_size\n","            db3 = torch.sum(dz3, dim=0) / batch_size\n","\n","            da2 = dz3 @ self.weights3.T\n","            dz2 = da2 * relu_derivative(self.z2)\n","            dw2 = self.a1.T @ dz2 / batch_size\n","            db2 = torch.sum(dz2, dim=0) / batch_size\n","\n","            da1 = dz2 @ self.weights2.T\n","            dz1 = da1 * relu_derivative(self.z1)\n","            dw1 = x.T @ dz1 / batch_size\n","            db1 = torch.sum(dz1, dim=0) / batch_size\n","\n","            # Update\n","            self.weights1 -= learning_rate * dw1\n","            self.bias1    -= learning_rate * db1\n","            self.weights2 -= learning_rate * dw2\n","            self.bias2    -= learning_rate * db2\n","            self.weights3 -= learning_rate * dw3\n","            self.bias3    -= learning_rate * db3\n","\n","        else:\n","            dz2 = self.a2 - y_one_hot\n","            dw2 = self.a1.T @ dz2 / batch_size\n","            db2 = torch.sum(dz2, dim=0) / batch_size\n","\n","            da1 = dz2 @ self.weights2.T\n","            dz1 = da1 * relu_derivative(self.z1)\n","            dw1 = x.T @ dz1 / batch_size\n","            db1 = torch.sum(dz1, dim=0) / batch_size\n","\n","            self.weights1 -= learning_rate * dw1\n","            self.bias1    -= learning_rate * db1\n","            self.weights2 -= learning_rate * dw2\n","            self.bias2    -= learning_rate * db2\n","\n","    def compute_loss(self, y_pred, y_true):\n","        batch_size = y_pred.shape[0]\n","        epsilon = 1e-9\n","        y_one_hot = torch.zeros_like(y_pred)\n","        y_one_hot[torch.arange(batch_size), y_true] = 1\n","        log_probs = torch.log(y_pred + epsilon)\n","        loss = -torch.sum(y_one_hot * log_probs) / batch_size\n","        return loss"],"metadata":{"id":"cTWLT6y78EvE","executionInfo":{"status":"ok","timestamp":1748095897496,"user_tz":-330,"elapsed":41,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### 3. Final Model Training (Best Configuration)"],"metadata":{"id":"d4ymMUCvljJm"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# Set parameters\n","input_size = 784\n","hidden_size = [128]\n","output_size = 10\n","learning_rate = 0.1\n","batch_size = 64\n","epochs = 5\n","\n","# Instantiate MLP\n","model = MLP(input_size, hidden_size, output_size)\n","\n","# Helper to convert dataset to tensors\n","def get_batch(dataset, idxs):\n","    images, labels = zip(*[dataset[i] for i in idxs])\n","    images = torch.stack([img for img in images])  # (B, 1, 28, 28)\n","    labels = torch.tensor(labels)\n","    return preprocess(images), labels\n","\n","# Training loop\n","for epoch in range(epochs):\n","    model_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    indices = torch.randperm(len(train_data))\n","    for i in range(0, len(train_data), batch_size):\n","        batch_idxs = indices[i:i+batch_size]\n","        x_batch, y_batch = get_batch(train_data, batch_idxs)\n","\n","        y_pred = model.forward(x_batch)\n","        loss = model.compute_loss(y_pred, y_batch)\n","        model.backward(x_batch, y_batch, learning_rate)\n","\n","        model_loss += loss.item()\n","        correct += (torch.argmax(y_pred, dim=1) == y_batch).sum().item()\n","        total += y_batch.size(0)\n","\n","    acc = 100 * correct / total\n","    print(f\"Epoch {epoch+1}: Loss = {model_loss:.4f}, Accuracy = {acc:.2f}%\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BMrTSJ1G8L9H","executionInfo":{"status":"ok","timestamp":1748095938135,"user_tz":-330,"elapsed":37298,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}},"outputId":"29efd57d-e8be-432d-994f-2a3c2a9e1502"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Loss = 583.5893, Accuracy = 73.10%\n","Epoch 2: Loss = 367.9654, Accuracy = 82.93%\n","Epoch 3: Loss = 324.5165, Accuracy = 85.11%\n","Epoch 4: Loss = 302.7966, Accuracy = 85.90%\n","Epoch 5: Loss = 286.4728, Accuracy = 86.71%\n"]}]},{"cell_type":"markdown","source":["### 3.1 Final Test Evaluation"],"metadata":{"id":"S4wlVbtPl1Su"}},{"cell_type":"code","source":["# Evaluation function to compute accuracy on a dataset (e.g., test or validation)\n","def evaluate(model, dataset):\n","    correct = 0  # Counter for correct predictions\n","    total = 0    # Counter for total predictions\n","\n","    # Process the dataset in batches\n","    for i in range(0, len(dataset), batch_size):\n","        # Create a batch of indices\n","        batch_idxs = list(range(i, min(i + batch_size, len(dataset))))\n","\n","        # Get the input images and labels for the current batch\n","        x_batch, y_batch = get_batch(dataset, batch_idxs)\n","\n","        # Disable gradient computation (inference mode)\n","        with torch.no_grad():\n","            y_pred = model.forward(x_batch)  # Forward pass\n","\n","        # Get predicted class labels by taking argmax across class scores\n","        preds = torch.argmax(y_pred, dim=1)\n","\n","        # Count how many predictions match the true labels\n","        correct += (preds == y_batch).sum().item()\n","        total += y_batch.size(0)  # Total number of samples in this batch\n","\n","    # Return accuracy as a percentage\n","    return 100 * correct / total\n","\n","# Evaluate the trained model on the test set\n","test_accuracy = evaluate(model, test_data)\n","\n","# Print final test accuracy\n","print(f\"✅ Final Test Accuracy with Best Hyperparameters: {test_accuracy:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dq1MaOYRAn1z","executionInfo":{"status":"ok","timestamp":1748095944497,"user_tz":-330,"elapsed":1196,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}},"outputId":"1106aa91-bfc2-4a90-bf8d-178f6dff678a"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Final Test Accuracy with Best Hyperparameters: 86.09%\n"]}]},{"cell_type":"markdown","source":["### 4. Best Configuration"],"metadata":{"id":"VUs84OaKrbgk"}},{"cell_type":"markdown","source":["### ✅ Best Hyperparameter Configuration\n","\n","| Parameter     | Value   |\n","|---------------|---------|\n","| Hidden Size   | 128     |\n","| Learning Rate | 0.1     |\n","| Batch Size    | 64      |\n","| Epochs        | 5       |\n","| Activation    | ReLU    |\n","| Layers        | 2 (Input → Hidden → Output) |\n","\n","**Final Test Accuracy:** 86.09%"],"metadata":{"id":"oPCw_ZsCE2bs"}},{"cell_type":"markdown","source":["### 5. Hyperparameter Experiments"],"metadata":{"id":"qGhFhC7hoclW"}},{"cell_type":"code","source":["def train_and_evaluate(hidden_sizes, learning_rate, batch_size, epochs=5):\n","    model = MLP(784, hidden_sizes, 10)\n","\n","    for epoch in range(epochs):\n","        indices = torch.randperm(len(train_data))\n","        for i in range(0, len(train_data), batch_size):\n","            batch_idxs = indices[i:i+batch_size]\n","            x_batch, y_batch = get_batch(train_data, batch_idxs)\n","            y_pred = model.forward(x_batch)\n","            loss = model.compute_loss(y_pred, y_batch)\n","            model.backward(x_batch, y_batch, learning_rate)\n","\n","    return evaluate(model, test_data)\n","\n","configs = [\n","    {\"hidden_sizes\": [128], \"lr\": 0.1, \"batch_size\": 64},\n","    {\"hidden_sizes\": [64], \"lr\": 0.1, \"batch_size\": 64},\n","    {\"hidden_sizes\": [128, 64], \"lr\": 0.1, \"batch_size\": 64},\n","    {\"hidden_sizes\": [128], \"lr\": 0.05, \"batch_size\": 64},\n","    {\"hidden_sizes\": [128], \"lr\": 0.1, \"batch_size\": 32},\n","    {\"hidden_sizes\": [256], \"lr\": 0.1, \"batch_size\": 64},\n","]\n","\n","results = []\n","for i, config in enumerate(configs):\n","    acc = train_and_evaluate(config[\"hidden_sizes\"], config[\"lr\"], config[\"batch_size\"])\n","    print(f\"Config {i+1}: Accuracy = {acc:.2f}%\")\n","    results.append((i+1, config, acc))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lKFesc_3Fwvj","executionInfo":{"status":"ok","timestamp":1748096185144,"user_tz":-330,"elapsed":233150,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}},"outputId":"571a9e12-1200-4720-dda2-0a90f047860e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Config 1: Accuracy = 86.09%\n","Config 2: Accuracy = 80.81%\n","Config 3: Accuracy = 78.94%\n","Config 4: Accuracy = 84.37%\n","Config 5: Accuracy = 86.44%\n","Config 6: Accuracy = 84.68%\n"]}]},{"cell_type":"markdown","source":["### 🔍 Hyperparameter Experiments\n","\n","| Config | Hidden Sizes | Learning Rate | Batch Size | Epochs | Test Accuracy (%) |\n","|--------|--------------|----------------|------------|--------|-------------------|\n","| 1      | [128]        | 0.1            | 64         | 5      | 86.09             |\n","| 2      | [64]         | 0.1            | 64         | 5      | 80.81             |\n","| 3      | [128, 64]    | 0.1            | 64         | 5      | 78.94             |\n","| 4      | [128]        | 0.05           | 64         | 5      | 84.37             |\n","| 5      | [128]        | 0.1            | 32         | 5      | 86.44             |\n","| 6      | [256]        | 0.1            | 64         | 5      | 84.68             |\n","\n","Although Config 5 achieved a slightly higher test accuracy (86.44%), we chose Config 1 as our best configuration since it was used for the final trained model submitted.Also, batch size 64 gives more stable training than 32 (more examples per update, less noise)."],"metadata":{"id":"WTi6DJGeLxSx"}},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NqarZ3wbzdFA","executionInfo":{"status":"ok","timestamp":1748098737042,"user_tz":-330,"elapsed":121,"user":{"displayName":"Suhas H G","userId":"04618837260270718075"}},"outputId":"5715941e-6021-44cb-adcb-6c856ef2196b"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]}]}